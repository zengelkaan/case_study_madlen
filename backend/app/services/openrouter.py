# openrouter.py - OpenRouter API client servisi
# OpenRouter API ile iletiÅŸim kuran servis sÄ±nÄ±fÄ±

import httpx  # Async HTTP client - OpenRouter API Ã§aÄŸrÄ±larÄ± iÃ§in
from typing import List, Dict, Any, Optional, AsyncGenerator  # Type hints
from opentelemetry import trace  # OpenTelemetry tracing - custom span'ler iÃ§in
from app.config import settings  # Ayarlardan API key alacaÄŸÄ±z

# Tracer oluÅŸtur - bu servis iÃ§in custom span'ler oluÅŸturmak Ã¼zere
tracer = trace.get_tracer(__name__)


class OpenRouterService:
    """
    OpenRouter API ile iletiÅŸim servisi
    AI modelleri listesi ve chat completion iÅŸlemlerini yÃ¶netir
    """
    
    def __init__(self):
        """
        Servis baÅŸlatÄ±cÄ± - API ayarlarÄ±nÄ± yÃ¼kle
        """
        self.api_key = settings.OPENROUTER_API_KEY  # .env'den API key al
        self.base_url = settings.OPENROUTER_BASE_URL  # Base URL (https://openrouter.ai/api/v1)
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",  # Bearer token ile yetkilendirme
            "Content-Type": "application/json",  # JSON formatÄ±nda veri gÃ¶nderiyoruz
        }  # Her request'te kullanÄ±lacak header'lar
    
    async def get_models(self, free_only: bool = True) -> List[Dict[str, Any]]:
        """
        OpenRouter'dan AI modellerini listele
        
        Args:
            free_only: True ise sadece Ã¼cretsiz modeller, False ise tÃ¼m modeller
        
        Returns:
            List[Dict]: Model listesi - her model bir dictionary
        """
        # Custom span oluÅŸtur - bu iÅŸlemi trace et
        with tracer.start_as_current_span("openrouter.get_models") as span:
            # Span'e attribute ekle - debug iÃ§in yararlÄ±
            span.set_attribute("openrouter.endpoint", "/models")  # Hangi endpoint Ã§aÄŸrÄ±ldÄ±
            span.set_attribute("openrouter.free_only", free_only)  # Sadece Ã¼cretsiz mi?
            
            async with httpx.AsyncClient() as client:  # Async HTTP client aÃ§
                try:
                    # OpenRouter models endpoint'ine GET request
                    response = await client.get(
                        f"{self.base_url}/models",  # https://openrouter.ai/api/v1/models
                        headers=self.headers,  # Authorization header ile
                        timeout=10.0  # 10 saniye timeout
                    )
                    response.raise_for_status()  # 4xx veya 5xx hatalarÄ±nda exception fÄ±rlat
                    
                    # Response'u JSON'a Ã§evir
                    data = response.json()
                    all_models = data.get("data", [])  # "data" anahtarÄ±ndaki model listesi
                    
                    # Modelleri iÅŸle - fiyat bilgilerini ekle
                    processed_models = []
                    for model in all_models:
                        # Fiyat bilgilerini al (string olarak gelir, float'a Ã§evir)
                        pricing = model.get("pricing", {})  # Pricing objesi
                        prompt_price = float(pricing.get("prompt", "0"))  # Prompt baÅŸÄ±na Ã¼cret ($)
                        completion_price = float(pricing.get("completion", "0"))  # Completion baÅŸÄ±na Ã¼cret ($)
                        
                        # Ãœcretsiz model kontrolÃ¼
                        is_free = (prompt_price == 0 and completion_price == 0)  # Her ikisi de 0 ise Ã¼cretsiz
                        
                        # EÄŸer sadece Ã¼cretsiz modeller istenmiÅŸse ve model Ã¼cretliyse, atla
                        if free_only and not is_free:
                            continue  # Bu modeli listeye ekleme
                        
                        # Ortalama maliyet hesapla (prompt + completion ortalamasÄ±, 1M token iÃ§in)
                        avg_cost = (prompt_price + completion_price) / 2 if not is_free else 0  # $/1M token
                        
                        # Vision desteÄŸi kontrolÃ¼ (description veya id'de "vision" geÃ§iyor mu?)
                        description = model.get("description", "").lower()  # AÃ§Ä±klama (kÃ¼Ã§Ã¼k harf)
                        model_id = model.get("id", "").lower()  # Model ID (kÃ¼Ã§Ã¼k harf)
                        model_name = model.get("name", "").lower()  # Model adÄ± (kÃ¼Ã§Ã¼k harf)
                        
                        # Vision keyword'lerini ara
                        supports_vision = any(
                            keyword in description or keyword in model_id or keyword in model_name
                            for keyword in ["vision", "image", "visual", "multimodal", "gpt-4o", "gpt-4-turbo", "claude-3"]
                        )  # Vision desteÄŸi var mÄ±?
                        
                        # Model bilgilerini ekle
                        processed_models.append({
                            "id": model.get("id"),  # Model ID - Ã¶rn: "mistralai/mistral-7b-instruct"
                            "name": model.get("name", model.get("id")),  # Model adÄ± - yoksa ID kullan
                            "description": model.get("description", ""),  # Model aÃ§Ä±klamasÄ±
                            "context_length": model.get("context_length", 4096),  # Max token sayÄ±sÄ±
                            "pricing": {
                                "prompt": prompt_price,  # Prompt baÅŸÄ±na Ã¼cret ($/1M token)
                                "completion": completion_price,  # Completion baÅŸÄ±na Ã¼cret ($/1M token)
                                "average": avg_cost,  # Ortalama maliyet ($/1M token)
                            },  # Fiyat bilgileri
                            "is_free": is_free,  # Ãœcretsiz mi?
                            "supportsVision": supports_vision,  # Vision desteÄŸi var mÄ±?
                        })
                    
                    # Span'e sonuÃ§ bilgisi ekle
                    span.set_attribute("openrouter.models_count", len(processed_models))  # KaÃ§ model dÃ¶ndÃ¼
                    span.set_attribute("openrouter.status", "success")  # Ä°ÅŸlem baÅŸarÄ±lÄ±
                    
                    return processed_models  # Model listesini dÃ¶ndÃ¼r
                    
                except httpx.HTTPError as e:  # HTTP hatalarÄ± (network, timeout, vb.)
                    # Span'e hata bilgisi ekle
                    span.set_attribute("openrouter.status", "error")  # Ä°ÅŸlem hatalÄ±
                    span.set_attribute("error.message", str(e))  # Hata mesajÄ±
                    span.record_exception(e)  # Exception'Ä± trace'e kaydet
                    
                    print(f"âŒ OpenRouter API HatasÄ±: {e}")  # Hata mesajÄ±nÄ± logla
                    return []  # BoÅŸ liste dÃ¶ndÃ¼r - frontend'e hata gÃ¶sterme yerine
    
    async def chat_completion(
        self,
        model: str,  # KullanÄ±lacak model ID - Ã¶rn: "mistralai/mistral-7b-instruct"
        messages: List[Dict[str, str]],  # Sohbet geÃ§miÅŸi - [{"role": "user", "content": "..."}]
        stream: bool = False  # Streaming mode - True ise cevabÄ± parÃ§a parÃ§a al
    ) -> Dict[str, Any]:
        """
        OpenRouter'a chat completion isteÄŸi gÃ¶nder
        Model'den cevap al (streaming veya normal)
        
        Args:
            model: KullanÄ±lacak AI model ID
            messages: Sohbet mesajlarÄ± - format: [{"role": "user/assistant", "content": "..."}]
            stream: Streaming mode aktif mi?
            
        Returns:
            Dict: Model'in cevabÄ± veya hata mesajÄ±
        """
        # Custom span oluÅŸtur - AI completion iÅŸlemini trace et
        with tracer.start_as_current_span("openrouter.chat_completion") as span:
            # Span'e attribute ekle - AI iÅŸlemi detaylarÄ±
            span.set_attribute("openrouter.model", model)  # Hangi model kullanÄ±ldÄ±
            span.set_attribute("openrouter.message_count", len(messages))  # KaÃ§ mesaj gÃ¶nderildi
            span.set_attribute("openrouter.stream", stream)  # Streaming mode var mÄ±
            
            async with httpx.AsyncClient() as client:  # Async HTTP client aÃ§
                try:
                    # Request payload oluÅŸtur
                    payload = {
                        "model": model,  # Hangi model kullanÄ±lacak
                        "messages": messages,  # Sohbet geÃ§miÅŸi
                        "stream": stream,  # Streaming aktif mi?
                    }
                    
                    # OpenRouter chat completion endpoint'ine POST request
                    response = await client.post(
                        f"{self.base_url}/chat/completions",  # https://openrouter.ai/api/v1/chat/completions
                        headers=self.headers,  # Authorization header
                        json=payload,  # Request body - JSON formatÄ±nda
                        timeout=60.0  # 60 saniye timeout (AI cevabÄ± iÃ§in daha uzun)
                    )
                    response.raise_for_status()  # Hata varsa exception fÄ±rlat
                    
                    # Response'u JSON'a Ã§evir
                    result = response.json()
                    
                    # Span'e baÅŸarÄ± bilgisi ekle
                    span.set_attribute("openrouter.status", "success")  # Ä°ÅŸlem baÅŸarÄ±lÄ±
                    # AI cevabÄ±nÄ±n uzunluÄŸu
                    if "choices" in result and len(result["choices"]) > 0:
                        content = result["choices"][0].get("message", {}).get("content", "")
                        span.set_attribute("openrouter.response_length", len(content))  # Cevap uzunluÄŸu
                    
                    return result  # Sonucu dÃ¶ndÃ¼r
                    
                except httpx.HTTPStatusError as e:  # HTTP status hatalarÄ± (404, 429, 500 vb.)
                    # Span'e hata bilgisi ekle
                    span.set_attribute("openrouter.status", "error")  # Ä°ÅŸlem hatalÄ±
                    span.set_attribute("error.message", str(e))  # Hata mesajÄ±
                    span.set_attribute("error.status_code", e.response.status_code)  # HTTP status code
                    span.record_exception(e)  # Exception'Ä± trace'e kaydet
                    
                    # KullanÄ±cÄ± dostu hata mesajÄ± oluÅŸtur - status code'a gÃ¶re
                    status_code = e.response.status_code
                    
                    if status_code == 429:
                        user_message = "Ã‡ok fazla istek gÃ¶nderildi. LÃ¼tfen 1-2 dakika bekleyip tekrar deneyin."
                    elif status_code == 404:
                        user_message = "Model bulunamadÄ±. LÃ¼tfen farklÄ± bir model seÃ§in."
                    elif status_code == 401:
                        user_message = "API anahtarÄ± geÃ§ersiz. LÃ¼tfen backend .env dosyasÄ±nÄ± kontrol edin."
                    elif status_code == 503:
                        user_message = "AI servisi ÅŸu an kullanÄ±lamÄ±yor. LÃ¼tfen daha sonra tekrar deneyin."
                    else:
                        user_message = f"Bir hata oluÅŸtu (Kod: {status_code}). LÃ¼tfen tekrar deneyin."
                    
                    print(f"âŒ Chat Completion HatasÄ± ({status_code}): {e}")  # DetaylÄ± hata logla
                    
                    # Hata durumunda user-friendly mesaj dÃ¶ndÃ¼r
                    return {
                        "error": True,  # Hata bayraÄŸÄ±
                        "message": user_message,  # KullanÄ±cÄ± dostu mesaj
                        "status_code": status_code  # Status code
                    }
                    
                except httpx.HTTPError as e:  # DiÄŸer HTTP hatalarÄ± (timeout, connection vb.)
                    # Span'e hata bilgisi ekle
                    span.set_attribute("openrouter.status", "error")
                    span.set_attribute("error.message", str(e))
                    span.record_exception(e)
                    
                    print(f"âŒ Chat Completion HatasÄ±: {e}")  # Hata logla
                    
                    # Hata durumunda user-friendly mesaj dÃ¶ndÃ¼r
                    return {
                        "error": True,  # Hata bayraÄŸÄ±
                        "message": "BaÄŸlantÄ± hatasÄ±. Ä°nternet baÄŸlantÄ±nÄ±zÄ± kontrol edin ve tekrar deneyin.",
                        "details": str(e)
                    }
    
    async def chat_completion_stream(
        self,
        model: str,  # KullanÄ±lacak model ID
        messages: List[Dict[str, str]]  # Sohbet geÃ§miÅŸi
    ) -> AsyncGenerator[str, None]:
        """
        OpenRouter'dan streaming chat completion
        Model'in cevabÄ±nÄ± parÃ§a parÃ§a (token token) al
        
        Args:
            model: KullanÄ±lacak AI model ID
            messages: Sohbet mesajlarÄ±
            
        Yields:
            str: Model'in cevabÄ±nÄ±n parÃ§alarÄ± (token'lar)
        """
        async with httpx.AsyncClient() as client:  # Async HTTP client aÃ§
            try:
                # Request payload - stream=True ile
                payload = {
                    "model": model,
                    "messages": messages,
                    "stream": True,  # Streaming mode aktif
                }
                
                # Streaming request - response'u parÃ§a parÃ§a oku
                async with client.stream(
                    "POST",
                    f"{self.base_url}/chat/completions",
                    headers=self.headers,
                    json=payload,
                    timeout=60.0
                ) as response:
                    response.raise_for_status()  # Hata kontrolÃ¼
                    
                    # Response'u satÄ±r satÄ±r oku (Server-Sent Events formatÄ±nda gelir)
                    async for line in response.aiter_lines():
                        if line.strip():  # BoÅŸ satÄ±rlarÄ± atla
                            # "data: " prefix'ini kaldÄ±r
                            if line.startswith("data: "):
                                data = line[6:]  # "data: " kÄ±smÄ±nÄ± kes
                                
                                # Stream sonu kontrolÃ¼
                                if data == "[DONE]":  # OpenRouter stream bitiÅŸi
                                    break  # DÃ¶ngÃ¼den Ã§Ä±k
                                
                                # JSON parse et ve content'i yield et
                                try:
                                    import json  # JSON parse iÃ§in
                                    chunk = json.loads(data)  # JSON'a Ã§evir
                                    # Delta iÃ§indeki content'i al
                                    content = chunk.get("choices", [{}])[0].get("delta", {}).get("content", "")
                                    if content:  # Content varsa
                                        yield content  # Token'Ä± gÃ¶nder
                                except json.JSONDecodeError:  # JSON parse hatasÄ±
                                    continue  # Bu satÄ±rÄ± atla, devam et
                                    
            except httpx.HTTPStatusError as e:  # HTTP status hatalarÄ± (404, 429, 500 vb.)
                # KullanÄ±cÄ± dostu hata mesajÄ± oluÅŸtur - status code'a gÃ¶re
                status_code = e.response.status_code  # HTTP status code
                
                if status_code == 429:
                    error_msg = "â³ Ã‡ok fazla istek gÃ¶nderildi.\n\nLÃ¼tfen 1-2 dakika bekleyip tekrar deneyin.\n\nÃœcretsiz API kullanÄ±mÄ±nda istek limiti vardÄ±r."
                elif status_code == 404:
                    error_msg = "âŒ Model bulunamadÄ± veya artÄ±k kullanÄ±lmÄ±yor.\n\nLÃ¼tfen model dropdown'Ä±ndan farklÄ± bir model seÃ§in."
                elif status_code == 401:
                    error_msg = "ğŸ”‘ API anahtarÄ± geÃ§ersiz.\n\nLÃ¼tfen backend/.env dosyasÄ±ndaki OPENROUTER_API_KEY'i kontrol edin."
                elif status_code == 503:
                    error_msg = "ğŸ”§ AI servisi ÅŸu an kullanÄ±lamÄ±yor.\n\nLÃ¼tfen birkaÃ§ dakika sonra tekrar deneyin."
                else:
                    error_msg = f"âŒ Beklenmeyen bir hata oluÅŸtu (HTTP {status_code}).\n\nLÃ¼tfen tekrar deneyin veya farklÄ± bir model seÃ§in."
                
                print(f"âŒ Streaming HatasÄ± ({status_code}): {e}")  # DetaylÄ± hata logla (backend console)
                yield error_msg  # KullanÄ±cÄ± dostu hata mesajÄ± - AI mesajÄ± olarak gÃ¶sterilecek
                
            except httpx.HTTPError as e:  # DiÄŸer HTTP hatalarÄ± (timeout, connection vb.)
                print(f"âŒ Streaming HatasÄ±: {e}")  # DetaylÄ± hata logla
                yield "âŒ BaÄŸlantÄ± hatasÄ± oluÅŸtu.\n\nÄ°nternet baÄŸlantÄ±nÄ±zÄ± kontrol edin ve tekrar deneyin."  # KullanÄ±cÄ± dostu mesaj


# Singleton instance - uygulama boyunca tek bir instance kullanÄ±lÄ±r
openrouter_service = OpenRouterService()

